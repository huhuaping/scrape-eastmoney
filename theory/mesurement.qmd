---
title: "测度指标"
---


::: {.callout-note}

**Google AI**:

I have got the intra-daily bid data set and the intra-daily deal price and volume data set for some stocks with its price hit the Limit Up price in some trade days. So how can I use this data set to predicate the possibility of limt up price hiting results of these stocks by using Machine Learning Models?

:::

 You have both intra-day bid data and intra-day deal data for stocks that hit limit up, which is ideal for building a predictive machine learning model. Here’s a breakdown of how to approach this, focusing on feature engineering, model selection, and evaluation:

## Data Preparation

- Data Loading: Load both the bid data and the deal (transaction) data for the stocks into your R environment. We'll assume you have separate data frames, let's say bid_df and deal_df, respectively.

- Time Alignment: Make sure both datasets have a common time variable (preferably in POSIXct format) for merging and analysis. You may need to convert time columns to POSIXct or similar formats as previously discussed.

- Label Creation: This is crucial. Create a binary label (limit_up_hit). For each trade day in your data, set limit_up_hit = 1 if the stock price hits limit up on that day, and set to 0 otherwise.

## Feature Engineering

### Feature Engineering from Bid Data

Focus on Intra-day Dynamics.

Here are some ideas, feel free to add more features based on your understanding of the market.

- Bid-Ask Spread: Calculate the average, standard deviation, minimum, and maximum bid-ask spread for a time period (e.g., every 5 mins).

- Bid-Ask Imbalance: Compute ratio between bid-side volume and ask-side volume.

- Bid-Ask Depth: Volume at different price levels on both sides of the order book.

- Bid/Ask Volume Change: Use time series difference function to observe the change of bid or ask volumes.

- Order Book Slope/Acceleration: How the bid/ask volumes and/or price are changing over time.

### Feature Engineering from Deal (Transaction) Data

- Price Change: Calculate the percentage change in price over short time frames(e.g. 1, 5 or 10mins)

- Volume Profile: Summarize the average volume and variance of the trade during certain periods of a trade day.

- Price Momentum: How fast the price is moving toward the Limit Up price in time series.

- Average trade size: Average volume of the trade.

### Time-Based Features

Add features that capture time-of-day effects, like

- Time since market open, time until close, and also consider morning, lunch, and afternoon sessions.

- Lagged Features: Use lagged values of features that may predict the future event, such as past moving average crossover of bid prices.

- Volatility: Utilize time series data to caculate the volatility.

### Other features

- Overall market situation.

- Sector trend.

- Stock's historical volatiltiy.

- Stock specific news and sentiment.

## Combining Data

Merge Datasets Merge bid data, deal data, and other features. You may merge them on time or aggregate data into a pre-defined time interval (e.g., 5-minute data). You need to choose the correct aggregation way.
* Handling Missing Values: Impute missing values carefully if there are any.


## Machine Learning Model Selection and Training

- Classification Problem: You’re trying to predict a binary outcome (limit_up_hit), making this a classification problem.

- Model Choices: Start with these options, and experiment as needed:

- Logistic Regression: Good starting point, interpretable, and works well when features are (somewhat) independent.

- Random Forest: Robust, handles non-linear relationships, and good with high-dimensional data.

- Gradient Boosting (XGBoost, LightGBM): Often a top performer, especially for complex datasets.

- Support Vector Machines (SVM): Effective when you have fewer features.

- Neural Networks: If you have a large dataset, neural nets may perform better but could be more complex.

- Splitting Data: Split your data into training, validation, and test sets.

- Training Set: Use this set to train your model.

- Validation Set: Use this set to tune model parameters.

- Test Set: Keep this set aside to evaluate the final performance.

- Cross-Validation: For more reliable performance estimates, use cross-validation techniques (k-fold, time-series CV).

- Model Training and Tuning: Use the training data and validation data to perform a grid search and tune your model parameters.

## Evaluation

- Metrics: Since it is likely that you have imbalanced dataset in which the limit_up_hit is mostly labeled as 0, so accuracy is not a good metric here. Instead, use the following metrics:

- Precision and Recall: Are important to evaluate model performance.

- F1-Score: The harmonic mean of precision and recall.

- Area Under the ROC Curve (AUC): Useful in evaluating classification performance.

- Confusion Matrix: Gives detailed breakdown (TP, TN, FP, FN) for your model to have deeper understanding of model's nature.


